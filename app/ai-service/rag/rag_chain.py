from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from core.ollama_client import get_llm

def build_rag_chain(vectordb, llm=None, k: int = 3):
    """
    Version ultra-simplifi√©e pour Phi3:mini avec debug
    """
    if llm is None:
        llm = get_llm()

    retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": k})
    print(f"üîç Retriever configur√© pour r√©cup√©rer {k} documents")

    # Template ULTRA-SIMPLE pour Phi3:mini
    template = """R√©ponds en utilisant UNIQUEMENT ces informations:

{context}

Question: {question}

R√©ponse (uniquement bas√©e sur les informations ci-dessus):"""

    qa_prompt = PromptTemplate.from_template(template)
    print("üìù Template de prompt ultra-simplifi√© configur√©")

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={
            "prompt": qa_prompt,
            "document_variable_name": "context"
        },
        return_source_documents=True
    )
    return qa_chain



