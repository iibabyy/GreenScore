from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from core.ollama_client import get_llm

def build_rag_chain(vectordb, llm=None, k: int = 8):
    """
    Version ultra-simplifi√©e pour Phi3:mini avec debug
    """
    if llm is None:
        llm = get_llm()

        retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": k})
        print(f"üîç Retriever configur√© pour r√©cup√©rer {k} documents")

        # Template am√©lior√©: autorise l'inf√©rence raisonn√©e si la r√©ponse n'est pas explicitement
        # pr√©sente dans les documents, demande d'indiquer les hypoth√®ses et un score de confiance.
        # Toujours citer les sources utilis√©es.
        template = """
Tu es Mr Green, un assistant expert en impacts environnementaux. Tu disposes uniquement de la
connaissance fournie ci-dessous (extraits de documents).

{context}

Instructions :
- Donne une r√©ponse concise √† la question.
- Si l'information demand√©e est explicitement pr√©sente dans les documents, r√©ponds en t'y
    appuyant et cite les documents (nom du fichier et extrait court si pertinent).
- Si l'information n'est PAS explicitement pr√©sente, fournis une estimation raisonn√©e ("best-effort")
    bas√©e sur les documents et connaissances connexes. Indique clairement les hypoth√®ses que tu fais
    (liste num√©rot√©e) et fournis une estimation de confiance sur 5 (ex: Confiance: 3/5).
- Ne fabrique pas de sources : indique "Aucune source directe trouv√©e" si applicable.
- Termine par une courte liste des sources cit√©es.

Question: {question}

R√©ponse:
"""

    qa_prompt = PromptTemplate.from_template(template)
    print("üìù Template am√©lior√© configur√©")

    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        retriever=retriever,
        chain_type="stuff",
        chain_type_kwargs={
            "prompt": qa_prompt,
            "document_variable_name": "context"
        },
        return_source_documents=True
    )
    return qa_chain



