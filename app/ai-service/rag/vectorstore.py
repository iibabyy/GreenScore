from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

def create_vectorstore(docs):
    print(f"üìÑ Cr√©ation de la vectorstore √† partir de {len(docs)} documents")
    
    # Chunks plus petits pour Phi3:mini
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=400,  # R√©duit de 750 √† 400
        chunk_overlap=50,  # R√©duit de 100 √† 50
        separators=["\n\n", "\n", ". ", "! ", "? ", " ", ""]
    )
    chunks = splitter.split_documents(docs)
    print(f"‚úÇÔ∏è Documents d√©coup√©s en {len(chunks)} chunks")

    # Mod√®le multilingue plus performant pour le fran√ßais
    print("üî§ Chargement du mod√®le d'embeddings multilingue...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'}  # Forc√© sur CPU
    )
      # Option 2: Si vous avez nomic-embed-text dans Ollama (recommand√©)
      # from langchain.embeddings import OllamaEmbeddings
      # embeddings = OllamaEmbeddings(
      #     model="nomic-embed-text",
      #     base_url="http://ollama:11434"
      # )

    print("üóÑÔ∏è Cr√©ation de la base vectorielle FAISS...")
    vectordb = FAISS.from_documents(chunks, embeddings)
    print("‚úÖ Base vectorielle cr√©√©e avec succ√®s!")
    return vectordb

