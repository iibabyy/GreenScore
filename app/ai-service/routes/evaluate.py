# ===== 5. evaluate.py (modifi√© avec query narrative fluide) =====
from fastapi import APIRouter, HTTPException
from fastapi.responses import StreamingResponse
from core.ollama_client import get_llm
import asyncio
import json
from rag.rag_chain import build_rag_chain
from rag.vectorstore_manager import VectorStoreManager
import time
import traceback
from core.ollama_client import ensure_ollama_warm
from core.config import settings
import os

MAX_EVAL_RETRIES = 3
RETRY_BACKOFF_BASE = 0.75

router = APIRouter()
vector_store_manager = VectorStoreManager()
vectordb = vector_store_manager.get_vectordb()

async def generate_streaming_response(product_description: str, debug: bool = False):
    print(f"\nüå± √âvaluation demand√©e pour: {product_description[:100]}...")
    print("‚ö° D√©marrage du processus d'√©valuation...")
    
    print("üîÑ Initialisation LLM (factory)...")
    start_init = time.time()
    llm = get_llm()
    init_time = time.time() - start_init
    print(f"‚úÖ LLM pr√™t (factory) en {init_time:.1f}s")
    
    print("üîÑ Construction de la cha√Æne RAG...")
    qa_chain = build_rag_chain(vectordb, llm=llm, k=settings.NUM_RETRIEVAL_DOCS, debug=debug)
    print("‚úÖ Cha√Æne RAG construite")

    # Query optimis√©e pour un texte fluide, narratif, avec int√©gration naturelle des scores
    query = f"""
Tu es un expert en analyse environnementale, mais tu t'adresses √† quelqu'un de curieux, pas √† un sp√©cialiste. 
Ton objectif est d'expliquer de mani√®re claire, engageante et naturelle l'impact √©cologique du produit suivant : {product_description}.

R√©dige le texte de mani√®re fluide et humaine , en int√©grant **tous les scores et donn√©es** dans les phrases, sans cr√©er de blocs ou listes.

- D√©cris chaque √©tape du cycle du produit (production, transport, emballage).  
- Ajoute des recommandations pratiques et concr√®tes √† la fin, int√©gr√©es dans le r√©cit.  
- Si une donn√©e n‚Äôest pas disponible, indique simplement "Non disponible".  
- √âvite le ton acad√©mique ou juridique : le texte doit se lire comme une conversation naturelle, engageante et accessible.
"""
    print(f"üîç Query construite: {query[:150]}...")

    if debug:
        print("üîß Mode debug : r√©cup√©ration des top-k sans appeler le LLM")
        print("üîÑ Ex√©cution de la recherche de documents similaires...")
        debug_res = qa_chain.run_with_debug(query)
        print("‚úÖ Recherche termin√©e")
        response = {
            "product": product_description,
            "debug": debug_res,
        }
        yield json.dumps(response)

    # Warmup Ollama (best-effort)
    warm_ok = ensure_ollama_warm()
    if not warm_ok:
        print("‚ö†Ô∏è Warmup Ollama non confirm√© (continuation quand m√™me)")

    start_time = time.time()
    last_error = None
    result = None
    total_time = 0
    source_docs = []

    for attempt in range(1, MAX_EVAL_RETRIES + 1):
        try:
            print(f"üîÑ Tentative d'√©valuation {attempt}/{MAX_EVAL_RETRIES}...")
            gen_start = time.time()
            result = await qa_chain.ainvoke({"query": query})
            gen_time = time.time() - gen_start
            print(f"‚úÖ √âvaluation r√©ussie! (temps: {gen_time:.1f}s)")
            total_time = time.time() - start_time
            if isinstance(result, dict):
                source_docs = result.get("source_documents", [])
                if "result" in result:
                    result = result["result"]
            break
        except Exception as e:
            last_error = e
            delay = round(RETRY_BACKOFF_BASE * attempt, 2)
            print(f"‚ùå √âchec tentative {attempt}/{MAX_EVAL_RETRIES}: {e} (retry in {delay}s)")
            await asyncio.sleep(delay)
    else:
        tb = traceback.format_exc(limit=3)
        error_response = {
            "error": "√âchec de g√©n√©ration apr√®s retries",
            "type": last_error.__class__.__name__ if last_error else "Unknown",
            "message": str(last_error),
            "trace": tb.splitlines(),
            "retries": MAX_EVAL_RETRIES,
        }
        yield json.dumps(error_response)

    answer = result or "<vide>"
    
    response = {
        "product": product_description,
        "evaluation": answer,
        "evaluation_time_seconds": round(total_time, 2),
        "source_documents": [
            {
                "source": doc.metadata.get("source", "unknown"), 
                "page": doc.metadata.get("page", None), 
                "text_snippet": doc.page_content[:400]
            }
            for doc in source_docs
        ],
        "debug_info": {
            "num_source_docs": len(source_docs),
            "evaluation_length": len(answer)
        }
    }
    
    print(f"‚è±Ô∏è √âvaluation termin√©e en {total_time:.2f}s")
    print(f"üìä R√©ponse d'√©valuation ({len(answer)} chars)")

    # Stream the response as chunks
    json_str = json.dumps(response)
    chunk_size = 1024
    for i in range(0, len(json_str), chunk_size):
        yield json_str[i:i+chunk_size]

from pydantic import BaseModel

class ProductRequest(BaseModel):
    product_description: str
    debug: bool = False

@router.post("/evaluate")
async def evaluate(request: ProductRequest):
    if len(request.product_description) > 1000:
        raise HTTPException(status_code=400, detail="Description produit trop longue (>1000 caract√®res)")
    return StreamingResponse(
        generate_streaming_response(request.product_description, request.debug),
        media_type="application/json"
    )